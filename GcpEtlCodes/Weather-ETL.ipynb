{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sys\n","from pyspark.sql import SparkSession, functions, types\n","from pyspark.sql.functions import when,lit,col,count,year,to_timestamp"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def main():\n","    observation_schema = types.StructType([\n","        types.StructField('station_id', types.StringType()),\n","        types.StructField('date', types.StringType()),\n","        types.StructField('observation', types.StringType()),\n","        types.StructField('value', types.IntegerType()),\n","        types.StructField('mflag', types.StringType()),\n","        types.StructField('qflag', types.StringType()),\n","        types.StructField('sflag', types.StringType()),\n","        types.StructField('obstime', types.StringType()),\n","    ])\n","\n","    # weather = spark.read.csv(inputs, schema=observation_schema)\n","    weather = spark.read.format('csv').schema(observation_schema)\\\n","        .option(\"recursiveFileLookup\", \"true\").load(\"gs://big-data-1-project-storage/ghcn/\")\n","    # checks for quality assurance test where blank indicates did not fail any quality assurance check\n","    weather_gflag = weather[weather['qflag'].isNull()]\n","    weather_station = weather_gflag[weather_gflag['station_id'].startswith('CA')]\n","    weather_filtered = weather_station.filter(weather_station['observation'].isin(['PRCP',\n","                                                                              'SNOW',\n","                                                                              'SNWD',\n","                                                                              'TMAX',\n","                                                                              'TMIN']))\n","\n","    weather_filtered = weather_filtered.filter(weather_filtered['date']>='19580101')\n","    weather_filtered = weather_filtered.groupBy(['station_id','date'])\\\n","        .pivot('observation') \\\n","        .max('value')\n","    weather_filtered = weather_filtered.withColumnRenamed('SNOW','SNOW(mm)')\\\n","        .withColumnRenamed('SNWD','SNWD(mm)').withColumnRenamed('PRCP','PRCP(tenths of mm)')\\\n","        .withColumnRenamed('TMAX','TMAX(tenths of degrees C)')\\\n","        .withColumnRenamed('TMIN','TMIN(tenths of degrees C)')\n","\n","    weather_filtered = weather_filtered.withColumn('TMAX(C)',weather_filtered['TMAX(tenths of degrees C)']/10)\\\n","        .withColumn('TMIN(C)',weather_filtered['TMIN(tenths of degrees C)']/10)\\\n","        .withColumn('PRCP(mm)',weather_filtered['PRCP(tenths of mm)']/10)\n","    weather_filtered = weather_filtered.select('station_id','date','TMAX(C)','TMIN(C)','PRCP(mm)','SNOW(mm)','SNWD(mm)')\n","\n","    weather_filtered.write.save(\"gs://big-data-1-project-storage/cleaned-data/weather_cleaned_1958Onwards.csv\",format='csv',header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    spark = SparkSession.builder.appName('weather etl transformation').getOrCreate()\n","    spark.sparkContext.setLogLevel('WARN')\n","    sc = spark.sparkContext\n","    main()\n","\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}